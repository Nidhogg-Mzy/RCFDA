# -*- coding: utf-8 -*-
"""5212.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12XNM6e8vV2V4ExatMbKkC0viElVTqqTs
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import GPT2Tokenizer
import torch
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader, TensorDataset
from torch.nn.utils.rnn import pad_sequence
from torch.optim import AdamW
from transformers import GPT2ForSequenceClassification, GPT2Config, get_linear_schedule_with_warmup
import datetime
import os
import datetime
import os
from torch.utils.tensorboard import SummaryWriter# Create an instance of the object 


current_dir = os.getcwd()
print(f'current dir is {current_dir}')

current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

os.environ['TRANSFORMERS_CACHE'] = '/data/clhuang/huggingface_cache'
writer = SummaryWriter(log_dir=f'{current_dir}/runs/gpt2_arxiv')

# outputs file name:
current_time = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
outputs_file_name = f"outputs/{current_time}.txt"
outputs_file_content = ""



# hyperparameter:
num_epochs = 10
hf_models_cache_dir = '/data/clhuang/huggingface_cache/models'
hf_tokenizer_cache_dir='/data/clhuang/huggingface_cache/tokenizers'
batch_size = 8
max_length = 512
file_name = f'{current_dir}/terminal_outputs/{current_time}.txt'

def print_and_store(current_file_content):
	global outputs_file_content 
	outputs_file_content += ('\n' + str(current_file_content))
	print(current_file_content)


# df = pd.read_csv('./arxiv100.csv') 
df = pd.read_csv(f'{current_dir}/Protoformer/data/arxiv100.csv')
le = LabelEncoder()
df['label'] = le.fit_transform(df['label']) # fit and transform. fit: get data's var, ave, etc. transform, normalisation

labels = df.label
texts = df.title + df.abstract
train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.1, random_state=1)



# tokenizor
tokenizer = GPT2Tokenizer.from_pretrained("gpt2", cache_dir=hf_models_cache_dir)
tokenizer.pad_token = tokenizer.eos_token
train_encoding = []
for text in train_texts:
    tokens = tokenizer.encode(text, truncation=True, max_length=max_length, padding=True)
    train_encoding.append(tokens)

test_encoding = []
for text in test_texts:
    tokens = tokenizer.encode(text, truncation=True, max_length=max_length, padding=True)
    test_encoding.append(tokens)






# # # load tokenized

# train_encoding=np.load('/data/clhuang/5212/bert_gpt_arxiv_imdb/gpt2_arxiv_encoding/train_encoding.npy', allow_pickle=True)
# train_encoding=train_encoding.tolist()

# test_encoding=np.load('/data/clhuang/5212/bert_gpt_arxiv_imdb/gpt2_arxiv_encoding/test_encoding.npy', allow_pickle=True)
# test_encoding=test_encoding.tolist()

# # # load tokenized
train_encoding=np.load(f'{current_dir}/gpt2_arxiv_encoding/train_encoding.npy', allow_pickle=True)
test_encoding=np.load(f'{current_dir}/gpt2_arxiv_encoding/test_encoding.npy', allow_pickle=True)



# sequence_list_tensors = [torch.tensor(seq) for seq in train_encoding]
# train_encoding = pad_sequence(sequence_list_tensors, batch_first=True)
# sequence_list_tensors = [torch.tensor(seq) for seq in test_encoding]
# test_encoding = pad_sequence(sequence_list_tensors, batch_first=True)

# # save encoding
# train_encoding_saved=np.array(train_encoding)
# np.save('/data/clhuang/5212/bert_gpt_arxiv_imdb/gpt2_arxiv_encoding/train_encoding.npy',train_encoding_saved)

# test_encoding_saved=np.array(test_encoding)
# np.save('/data/clhuang/5212/bert_gpt_arxiv_imdb/gpt2_arxiv_encoding/test_encoding.npy',test_encoding_saved)
# # end


train_labels_tensor = torch.zeros(len(train_labels), 10)
for i, label in enumerate(train_labels):
    train_labels_tensor[i, label] = 1 # for each instance, set the label to 1. other will remain 0

test_labels_tensor = torch.zeros(len(test_labels), 10)
for i, label in enumerate(test_labels):
    test_labels_tensor[i, label] = 1


# data_loader
class TensorDataset(Dataset):
    def __init__(self, data_tensor, target_tensor):
        super().__init__()
        self.data = data_tensor
        self.target = target_tensor

    def __getitem__(self, index):
        return self.data[index], self.target[index]

    def __len__(self):
        return len(self.data)

train_dataset = TensorDataset(train_encoding, train_labels_tensor)
test_dataset = TensorDataset(test_encoding, test_labels_tensor)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

print_and_store(train_loader)
print_and_store('Created `train_dataloader` with %d batches!'%len(train_loader))
# end

# cuda
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print_and_store('the device is: ' + str(device))
# end

# model
model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path="gpt2", num_labels=10, cache_dir=hf_models_cache_dir)
model = GPT2ForSequenceClassification.from_pretrained("gpt2", config = model_config, cache_dir=hf_models_cache_dir)
model.config.pad_token_id = model.config.eos_token_id
model.to(device)
# end

# hyper para + optimizer/scheduler
epochs = num_epochs
optimizer = AdamW(model.parameters(),lr = 2e-5, eps = 1e-8)

total_steps = len(train_loader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)
# end



print_and_store('======== TRAINING ====================')
total_loss = 0
step = 0
for epoch in range(epochs):
    acc = 0
    for batch in tqdm(train_loader, total=len(train_loader)):
        
        optimizer.zero_grad()
        inputs = {'input_ids': batch[0].to(device), 'labels': batch[1].to(device)}
        outputs = model(**inputs)
        logits = outputs.logits

        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        step += 1
        writer.add_scalar('Loss/train', loss, step)

        max_indices = logits.argmax(dim=1)
        for i in range(len(batch[1])):
            ind = int (max_indices[i])
            if batch[1][i][ind] == 1:
                acc = acc +1
        # ind = np.argmax(max_indices, axis=1) 
        # acc += np.sum(batch[1][np.arange(len(batch[1])), max_indices] == 1) # vectorization 
    
    accuracy = acc/(len(train_loader) * batch_size)
    total_loss = total_loss/len(train_loader)
    print_and_store(f"epoch{epoch+1}  loss = {total_loss}")
    print_and_store(f"epoch{epoch+1}  accuracy = {accuracy}")

    writer.add_scalar('Accuracy/train', accuracy, epoch)
torch.save(model.state_dict(), f'{current_dir}/ckpts/arxiv_{current_time}.ckpt')

print_and_store('======== TESTING ====================')
# load ckpts

checkpoint_path = f'{current_dir}/ckpts/arxiv_2023-12-10_02-39-56.ckpt'
# checkpoint = torch.load(checkpoint_path)
# print(checkpoint.keys())
# model.load_state_dict(checkpoint)


# testing
with torch.no_grad():
    # for batch in tqdm(test_loader, total=len(test_loader)):
    acc = 0
    for batch in test_loader:
        inputs = {'input_ids': batch[0].to(device), 'labels': batch[1].to(device)}
        outputs = model(**inputs)
        logits = outputs.logits
        max_indices = logits.argmax(dim=1)
        for i in range(len(batch[1])):
            ind = int (max_indices[i])
            if batch[1][i][ind] == 1:
                acc = acc +1
        # print(f'batch is {batch}')
        # break
    
    accuracy = acc/(len(test_loader) * batch_size) 
    writer.add_scalar('Accuracy/test', accuracy)
    print_and_store(f"Testing accuracy = {accuracy}")

with open(file_name, 'w') as file:
    file.write(outputs_file_content)