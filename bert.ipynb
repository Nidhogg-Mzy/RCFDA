{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":801,"status":"ok","timestamp":1681952742365,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"4jELcPqGCsKz","outputId":"9cc0aa88-9b35-4cdd-8001-085181d8a245"},"outputs":[],"source":["import os\n","if not os.path.exists('ArXiv-10.zip'):\n","  os.system('wget https://github.com/ashfarhangi/Protoformer/raw/main/data/ArXiv-10.zip')\n","  os.system('unzip ArXiv-10.zip')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1681952742366,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"N2rPwnOeDF9R"},"outputs":[],"source":["import argparse\n","\n","import pandas as pd\n","import os\n","import csv\n","import numpy as np\n","import re\n","import string\n","\n","mypath = ''"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1681952742366,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"GCFwEdfdF9WS"},"outputs":[],"source":["def preprocess(text):\n","    \n","    if text == '':\n","        return text\n","    \n","    # remove extra commas and semi-colons\n","    text = text.replace(\".,\",\".\")\n","    text = text.replace(\"..\",\".\")\n","    text = text.replace(\".;\",\".\")\n","    \n","    text = text.replace(\"!,\",\"!\")\n","    text = text.replace(\"!.\",\"!\")\n","    text = text.replace(\"!;\",\"!\")\n","    \n","    text = text.replace(\"?,\",\"!\")\n","    text = text.replace(\"?.\",\"!\")\n","    text = text.replace(\"?;\",\"!\")\n","    \n","    text = text.replace(\". ,\",\".\")\n","    text = text.replace(\". .\",\".\")\n","    text = text.replace(\". ;\",\".\")\n","    \n","    text = text.replace(\"! ,\",\"!\")\n","    text = text.replace(\"! .\",\"!\")\n","    text = text.replace(\"! ;\",\"!\")\n","    \n","    text = text.replace(\"? ,\",\"!\")\n","    text = text.replace(\"? .\",\"!\")\n","    text = text.replace(\"? ;\",\"!\")\n","    \n","    # Other common issues\n","    \n","    # remove first character if it is a punctuation\n","    if text[0] in string.punctuation:\n","        text = text[1:]\n","    \n","    # remove extra commas in text\n","    text = re.sub(r'[.]+[\\n]+[,]',\".\\n\", text)\n","    \n","    # remove extra semi-colons in text\n","    text = re.sub(r'[.]+[\\n]+[;]',\".\\n\", text)\n","    \n","    # Replace new line with space\n","    text = text.replace(\"\\n\",\" \")\n","    \n","    # Replace tab with space\n","    text = text.replace(\"\\t\",\" \")\n","    \n","    # Remove random new line + comma\n","    text = text.replace(\"\\n,\",\"\")\n","    \n","    # Replace multiple spaces with a single space\n","    text = re.sub(' +', ' ', text)\n","    \n","    # Remove trailing characters if it does not end with .\n","    while text[-1] != '.' and len(text)>2:\n","        text = text[:-2]\n","        \n","    # Remove initial characters if they are space or puctuation\n","    while text[0] in string.punctuation or text[0] == ' ':\n","        text = text[1:]\n","        if text == '':\n","            return text\n","    \n","    return text"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":17266,"status":"ok","timestamp":1681952759628,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"EKLXypQTCxHu"},"outputs":[],"source":["arxiv10 = pd.read_csv(mypath + 'arxiv100.csv')\n","arxiv10['abstract'] = arxiv10['abstract'].map(lambda x:preprocess(x))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":129,"status":"ok","timestamp":1681952759746,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"_Lk6noAUDIVu","outputId":"0c017b94-5fa8-4189-a2c4-f87da04d6299"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>abstract</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Pre-He White Dwarfs in Eclipsing Binaries....</td>\n","      <td>We report the first $BV$ light curves and high...</td>\n","      <td>astro-ph</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A Possible Origin of kHZ QPOs in Low-Mass X-ra...</td>\n","      <td>A possible origin of kHz QPOs in low-mass X-ra...</td>\n","      <td>astro-ph</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The effects of driving time scales on heating ...</td>\n","      <td>Context. The relative importance of AC and DC ...</td>\n","      <td>astro-ph</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A new hard X-ray selected sample of extreme hi...</td>\n","      <td>Extreme high-energy peaked BL Lac objects (EHB...</td>\n","      <td>astro-ph</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The baryon cycle of Seven Dwarfs with superbub...</td>\n","      <td>We present results from a high-resolution, cos...</td>\n","      <td>astro-ph</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               title  \\\n","0  The Pre-He White Dwarfs in Eclipsing Binaries....   \n","1  A Possible Origin of kHZ QPOs in Low-Mass X-ra...   \n","2  The effects of driving time scales on heating ...   \n","3  A new hard X-ray selected sample of extreme hi...   \n","4  The baryon cycle of Seven Dwarfs with superbub...   \n","\n","                                            abstract     label  \n","0  We report the first $BV$ light curves and high...  astro-ph  \n","1  A possible origin of kHz QPOs in low-mass X-ra...  astro-ph  \n","2  Context. The relative importance of AC and DC ...  astro-ph  \n","3  Extreme high-energy peaked BL Lac objects (EHB...  astro-ph  \n","4  We present results from a high-resolution, cos...  astro-ph  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["arxiv10.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1681952759746,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"yFcaDbBoUsAq"},"outputs":[],"source":["arxiv10 = arxiv10.dropna()"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":193,"status":"ok","timestamp":1681952759937,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"e84HQC3eYm-J","outputId":"690db82e-2cfc-4c49-8500-c356f4d21163"},"outputs":[{"data":{"text/plain":["(100000, 3)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["arxiv10.dropna().shape"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9460,"status":"ok","timestamp":1681952769396,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"ztZYCKCwK1Eh"},"outputs":[],"source":["dataset = arxiv10\n","dataset = dataset.dropna()\n","dataset.to_csv(mypath +'dataset.csv',index=False)\n","dataset = pd.read_csv(mypath + 'dataset.csv')\n","\n","\n","while dataset.shape!=dataset.dropna().shape:\n","  dataset = dataset.dropna()\n","  dataset.to_csv(mypath +'dataset.csv',index=False)\n","  dataset = pd.read_csv(mypath + 'dataset.csv')"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1681952769397,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"GBt2A9eVDVpQ"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zmengaf/miniconda3/envs/rcfda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n","  return bound(*args, **kwds)\n"]}],"source":["train, val, test = \\\n","              np.split(dataset.sample(frac=1, random_state=2023), \n","                       [int(.7*len(dataset)), int(.85*len(dataset))])"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3137,"status":"ok","timestamp":1681952772523,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"zuo8GwLFDyVy"},"outputs":[],"source":["train.to_csv(mypath +'train.csv',index=False)\n","test.to_csv(mypath +'test.csv',index=False)\n","val.to_csv(mypath +'val.csv',index=False)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1681952772523,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"9OkcCyGwEMNZ","outputId":"c0a28f97-839b-4d10-ab4b-38ad7c03057b"},"outputs":[{"name":"stdout","output_type":"stream","text":["67548\n","14475\n","14475\n"]}],"source":["print(len(train))\n","print(len(val))\n","print(len(test))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":685,"status":"ok","timestamp":1681952773206,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"vzdQOzfnY7U0","outputId":"31ad4d37-f357-4766-b210-fa8dc8bb16c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["67548\n","14475\n","14475\n"]}],"source":["print(len(train.dropna()))\n","print(len(val.dropna()))\n","print(len(test.dropna()))"]},{"cell_type":"markdown","metadata":{"id":"ruf1ytoKGjtA"},"source":["Train/Test/Validation sets are quite balanced."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1681952773206,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"S8SowD25FtPv","outputId":"e91c5da9-6795-4e24-eb04-f7fe04b289d0"},"outputs":[{"data":{"text/plain":["label\n","astro-ph    6884\n","cond-mat    6849\n","cs          6924\n","eess        6884\n","hep-ph      6629\n","hep-th      6699\n","math        6348\n","physics     6720\n","quant-ph    6746\n","stat        6865\n","dtype: int64"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train.groupby(['label']).size()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1681952773206,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"CI2LlvhYFy4X","outputId":"b7231c25-aaca-4665-93c0-135432537135"},"outputs":[{"data":{"text/plain":["label\n","astro-ph    1529\n","cond-mat    1505\n","cs          1469\n","eess        1435\n","hep-ph      1428\n","hep-th      1391\n","math        1355\n","physics     1483\n","quant-ph    1477\n","stat        1403\n","dtype: int64"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["test.groupby(['label']).size()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1681952773206,"user":{"displayName":"Xiaochen Long","userId":"02430345621122394718"},"user_tz":300},"id":"nX7LB6FiFzcJ","outputId":"0ebc669a-7b2c-4a35-c091-3b2dda0f54e4"},"outputs":[{"data":{"text/plain":["label\n","astro-ph    1456\n","cond-mat    1414\n","cs          1431\n","eess        1507\n","hep-ph      1453\n","hep-th      1414\n","math        1305\n","physics     1525\n","quant-ph    1431\n","stat        1539\n","dtype: int64"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["val.groupby(['label']).size()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["train = pd.read_csv(mypath + 'train.csv')\n","test = pd.read_csv(mypath + 'test.csv')\n","val = pd.read_csv(mypath + 'val.csv')"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import json\n","id2label = ['physics', 'hep-ph', 'eess', 'astro-ph', 'hep-th', 'quant-ph', 'stat', 'math', 'cond-mat', 'cs']\n","label2id = {'physics': 0, 'hep-ph': 1, 'eess': 2, 'astro-ph': 3, 'hep-th': 4, 'quant-ph': 5, 'stat': 6, 'math': 7, 'cond-mat': 8, 'cs': 9}\n","\n","with open(\"id2label.json\", \"w\") as f:\n","    json.dump(id2label, f)\n","    \n","with open(\"label2id.json\", \"w\") as f:\n","    json.dump(label2id, f)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["train['label'] = [label2id[key] for key in train['label']]\n","test['label'] = [label2id[key] for key in test['label']]\n","val['label'] = [label2id[key] for key in val['label']]\n","\n","train = train.drop(['title'], axis = 1)\n","test = test.drop(['title'], axis = 1)\n","val = val.drop(['title'], axis = 1)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["train.to_csv('train_abstract_ilabel.csv')\n","val.to_csv('val_abstract_ilabel.csv')\n","test.to_csv('test_abstract_ilabel.csv')"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['physics', 'hep-ph', 'eess', 'astro-ph', 'hep-th', 'quant-ph', 'stat', 'math', 'cond-mat', 'cs']\n","{'physics': 0, 'hep-ph': 1, 'eess': 2, 'astro-ph': 3, 'hep-th': 4, 'quant-ph': 5, 'stat': 6, 'math': 7, 'cond-mat': 8, 'cs': 9}\n"]}],"source":["import json\n","\n","label2id = {}\n","id2label = {}\n","\n","with open(\"id2label.json\", \"r\") as f:\n","    id2label = json.load(f)\n","    \n","with open(\"label2id.json\", \"r\") as f:\n","    label2id = json.load(f)\n","\n","print(id2label)\n","print(label2id)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","max_length = 512\n","padding = \"max_length\"\n","truncation = True\n","model_name = \"bert-base-uncased\"\n","num_epoch = 100\n","batch_size = 10\n","num_labels = 10\n","add_special_tokens=True\n","return_attention_mask=True\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zmengaf/miniconda3/envs/rcfda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels).to(device)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class bert_dataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length, add_special_tokens, padding, return_attention_mask, truncation):\n","        self.texts = data['abstract']\n","        self.labels = data['label']\n","        self.tokenizer = tokenizer\n","        input_ids = []\n","        attention_masks = []\n","\n","        for text in self.texts:\n","            encoded_dict = tokenizer.encode_plus(\n","                text,\n","                add_special_tokens=add_special_tokens,\n","                max_length=max_length,\n","                padding=padding,\n","                return_attention_mask=return_attention_mask,\n","                return_tensors='pt',\n","                truncation=truncation\n","            )\n","            input_ids.append(encoded_dict['input_ids'])\n","            attention_masks.append(encoded_dict['attention_mask'])\n","\n","        self.input_ids = torch.cat(input_ids, axis=0)\n","        self.attention_masks = torch.cat(attention_masks, axis=0)\n","        self.labels = torch.tensor(self.labels, dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        return {'input_ids': self.input_ids[idx],\n","                'attention_mask': self.attention_masks[idx],\n","                'labels': self.labels[idx]}"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/data/zmengaf/5212/RCFDA/bert.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mtrain_abstract_ilabel.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m bert_dataset(train_data, tokenizer, max_length\u001b[39m=\u001b[39;49mmax_length, add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                               padding\u001b[39m=\u001b[39;49mpadding, return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask, truncation\u001b[39m=\u001b[39;49mtruncation)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m val_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mval_abstract_ilabel.csv\u001b[39m\u001b[39m'\u001b[39m)\n","\u001b[1;32m/data/zmengaf/5212/RCFDA/bert.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m attention_masks \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtexts:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     encoded_dict \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         text,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     input_ids\u001b[39m.\u001b[39mappend(encoded_dict[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     attention_masks\u001b[39m.\u001b[39mappend(encoded_dict[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m])\n","File \u001b[0;32m~/miniconda3/envs/rcfda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2781\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2771\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2772\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2773\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2774\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2778\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2779\u001b[0m )\n\u001b[0;32m-> 2781\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2782\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2783\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2784\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2785\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2786\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2787\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2788\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2789\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2790\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2791\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2792\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2793\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2794\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2795\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2796\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2797\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2798\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2799\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2800\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/rcfda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:517\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    496\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    497\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    515\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    516\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 517\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    518\u001b[0m         batched_input,\n\u001b[1;32m    519\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    520\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    521\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    522\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    523\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    524\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    525\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    526\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    527\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    528\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    529\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    530\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    531\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    532\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    533\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    534\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    535\u001b[0m     )\n\u001b[1;32m    537\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n","File \u001b[0;32m~/miniconda3/envs/rcfda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:493\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    492\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n","File \u001b[0;32m~/miniconda3/envs/rcfda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:199\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    192\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m     data: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m     n_sequences: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    198\u001b[0m ):\n\u001b[0;32m--> 199\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data)\n\u001b[1;32m    201\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(encoding, EncodingFast):\n\u001b[1;32m    202\u001b[0m         encoding \u001b[39m=\u001b[39m [encoding]\n","File \u001b[0;32m~/miniconda3/envs/rcfda/lib/python3.10/collections/__init__.py:1094\u001b[0m, in \u001b[0;36mUserDict.__init__\u001b[0;34m(self, dict, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1093\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mdict\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1094\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(\u001b[39mdict\u001b[39;49m)\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[1;32m   1096\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(kwargs)\n","File \u001b[0;32m~/miniconda3/envs/rcfda/lib/python3.10/_collections_abc.py:991\u001b[0m, in \u001b[0;36mMutableMapping.update\u001b[0;34m(self, other, **kwds)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    989\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 991\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, other\u001b[39m=\u001b[39m(), \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    992\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m        If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m        If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[39m        In either case, this is followed by: for k, v in F.items(): D[k] = v\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Mapping):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_data = pd.read_csv('train_abstract_ilabel.csv')\n","train_dataset = bert_dataset(train_data, tokenizer, max_length=max_length, add_special_tokens=add_special_tokens, \n","                              padding=padding, return_attention_mask=return_attention_mask, truncation=truncation)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","val_data = pd.read_csv('val_abstract_ilabel.csv')\n","val_dataset = bert_dataset(val_data, tokenizer, max_length=max_length, add_special_tokens=add_special_tokens, \n","                              padding=padding, return_attention_mask=return_attention_mask, truncation=truncation)\n","val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","\n","test_data = pd.read_csv('test_abstract_ilabel.csv')\n","test_dataset = bert_dataset(test_data, tokenizer, max_length=max_length, add_special_tokens=add_special_tokens, \n","                              padding=padding, return_attention_mask=return_attention_mask, truncation=truncation)\n","test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zmengaf/miniconda3/envs/rcfda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"text/html":["Finishing last run (ID:ngou5ss5) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"]},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_acc</td><td>▁▄▅▇▇██</td></tr><tr><td>train_loss</td><td>█▅▄▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▇▇██</td></tr><tr><td>val_loss</td><td>▂▁▂▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>train_acc</td><td>9.48557</td></tr><tr><td>train_loss</td><td>0.14963</td></tr><tr><td>val_acc</td><td>9.48557</td></tr><tr><td>val_loss</td><td>0.18358</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">arxiv_bert</strong> at: <a href='https://wandb.ai/zmengaf/rcfda/runs/ngou5ss5' target=\"_blank\">https://wandb.ai/zmengaf/rcfda/runs/ngou5ss5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20231208_172355-ngou5ss5/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:ngou5ss5). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/data/zmengaf/5212/RCFDA/wandb/run-20231208_192258-7bzlmyzz</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/zmengaf/rcfda/runs/7bzlmyzz' target=\"_blank\">arxiv_bert</a></strong> to <a href='https://wandb.ai/zmengaf/rcfda' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/zmengaf/rcfda' target=\"_blank\">https://wandb.ai/zmengaf/rcfda</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/zmengaf/rcfda/runs/7bzlmyzz' target=\"_blank\">https://wandb.ai/zmengaf/rcfda/runs/7bzlmyzz</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 0: train_loss = 0.1300, train_acc = 9.5537\n","8.0\n","16.0\n","21.0\n","29.0\n","38.0\n","47.0\n","53.0\n","61.0\n","70.0\n","78.0\n","84.0\n","93.0\n","103.0\n","110.0\n","118.0\n","126.0\n","135.0\n","144.0\n","152.0\n","162.0\n","170.0\n","178.0\n","186.0\n","192.0\n","201.0\n","209.0\n","218.0\n","227.0\n","233.0\n","241.0\n","248.0\n","258.0\n","267.0\n","273.0\n","280.0\n","289.0\n","297.0\n","306.0\n","314.0\n","322.0\n","332.0\n","339.0\n","348.0\n","356.0\n","365.0\n","373.0\n","379.0\n","387.0\n","396.0\n","404.0\n","413.0\n","421.0\n","428.0\n","435.0\n","443.0\n","452.0\n","459.0\n","467.0\n","474.0\n","482.0\n","489.0\n","498.0\n","506.0\n","515.0\n","522.0\n","529.0\n","538.0\n","545.0\n","554.0\n","562.0\n","570.0\n","578.0\n","587.0\n","597.0\n","605.0\n","614.0\n","622.0\n","630.0\n","637.0\n","645.0\n","654.0\n","659.0\n","666.0\n","673.0\n","679.0\n","684.0\n","691.0\n","701.0\n","708.0\n","715.0\n","724.0\n","733.0\n","740.0\n","747.0\n","755.0\n","763.0\n","773.0\n","781.0\n","787.0\n","795.0\n","802.0\n","811.0\n","819.0\n","827.0\n","834.0\n","839.0\n","848.0\n","856.0\n","863.0\n","871.0\n","881.0\n","889.0\n","895.0\n","903.0\n","910.0\n","917.0\n","927.0\n","936.0\n","945.0\n","953.0\n","960.0\n","966.0\n","973.0\n","978.0\n","984.0\n","994.0\n","1001.0\n","1009.0\n","1017.0\n","1026.0\n","1035.0\n","1044.0\n","1053.0\n","1062.0\n","1070.0\n","1078.0\n","1087.0\n","1094.0\n","1102.0\n","1110.0\n","1120.0\n","1128.0\n","1136.0\n","1143.0\n","1149.0\n","1155.0\n","1163.0\n","1170.0\n","1175.0\n","1185.0\n","1192.0\n","1199.0\n","1207.0\n","1215.0\n","1222.0\n","1229.0\n","1237.0\n","1243.0\n","1250.0\n","1260.0\n","1269.0\n","1276.0\n","1285.0\n","1292.0\n","1299.0\n","1309.0\n","1317.0\n","1324.0\n","1330.0\n","1337.0\n","1345.0\n","1352.0\n","1360.0\n","1367.0\n","1376.0\n","1385.0\n","1395.0\n","1403.0\n","1411.0\n","1420.0\n","1428.0\n","1437.0\n","1445.0\n","1450.0\n","1456.0\n","1464.0\n","1471.0\n","1478.0\n","1487.0\n","1496.0\n","1505.0\n","1512.0\n","1519.0\n","1526.0\n","1532.0\n","1538.0\n","1547.0\n","1556.0\n","1566.0\n","1575.0\n","1583.0\n","1593.0\n","1601.0\n","1611.0\n","1617.0\n","1625.0\n","1632.0\n","1641.0\n","1649.0\n","1658.0\n","1664.0\n","1673.0\n","1682.0\n","1690.0\n","1697.0\n","1706.0\n","1716.0\n","1721.0\n","1728.0\n","1736.0\n","1742.0\n","1750.0\n","1758.0\n","1767.0\n","1775.0\n","1784.0\n","1793.0\n","1802.0\n","1811.0\n","1818.0\n","1828.0\n","1836.0\n","1845.0\n","1854.0\n","1863.0\n","1872.0\n","1882.0\n","1890.0\n","1898.0\n","1907.0\n","1916.0\n","1926.0\n","1934.0\n","1942.0\n","1950.0\n","1957.0\n","1964.0\n","1974.0\n","1982.0\n","1989.0\n","1999.0\n","2005.0\n","2009.0\n","2017.0\n","2025.0\n","2034.0\n","2042.0\n","2050.0\n","2057.0\n","2065.0\n","2072.0\n","2081.0\n","2089.0\n","2097.0\n","2106.0\n","2113.0\n","2120.0\n","2130.0\n","2139.0\n","2146.0\n","2153.0\n","2162.0\n","2171.0\n","2179.0\n","2188.0\n","2194.0\n","2203.0\n","2212.0\n","2221.0\n","2230.0\n","2239.0\n","2248.0\n","2258.0\n","2266.0\n","2274.0\n","2282.0\n","2289.0\n","2298.0\n","2306.0\n","2314.0\n","2323.0\n","2332.0\n","2339.0\n","2347.0\n","2355.0\n","2365.0\n","2371.0\n","2380.0\n","2389.0\n","2396.0\n","2406.0\n","2414.0\n","2422.0\n","2430.0\n","2438.0\n","2445.0\n","2451.0\n","2457.0\n","2467.0\n","2476.0\n","2485.0\n","2492.0\n","2501.0\n","2506.0\n","2514.0\n","2521.0\n","2531.0\n","2540.0\n","2548.0\n","2555.0\n","2562.0\n","2570.0\n","2579.0\n","2589.0\n","2597.0\n","2605.0\n","2613.0\n","2621.0\n","2628.0\n","2636.0\n","2642.0\n","2651.0\n","2661.0\n","2670.0\n","2678.0\n","2685.0\n","2694.0\n","2702.0\n","2712.0\n","2720.0\n","2726.0\n","2733.0\n","2743.0\n","2751.0\n","2758.0\n","2768.0\n","2777.0\n","2787.0\n","2797.0\n","2801.0\n","2810.0\n","2818.0\n","2825.0\n","2832.0\n","2842.0\n","2848.0\n","2855.0\n","2865.0\n","2874.0\n","2882.0\n","2890.0\n","2898.0\n","2904.0\n","2912.0\n","2921.0\n","2929.0\n","2936.0\n","2943.0\n","2950.0\n","2958.0\n","2966.0\n","2972.0\n","2980.0\n","2990.0\n","2994.0\n","3003.0\n","3011.0\n","3020.0\n","3028.0\n","3038.0\n","3045.0\n","3055.0\n","3063.0\n","3070.0\n","3078.0\n","3084.0\n","3094.0\n","3103.0\n","3111.0\n","3116.0\n","3122.0\n","3130.0\n","3137.0\n","3146.0\n","3152.0\n","3159.0\n","3168.0\n","3174.0\n","3182.0\n","3188.0\n","3195.0\n","3205.0\n","3212.0\n","3219.0\n","3227.0\n","3236.0\n","3246.0\n","3255.0\n","3261.0\n","3267.0\n","3274.0\n","3282.0\n","3291.0\n","3301.0\n","3306.0\n","3315.0\n","3321.0\n","3329.0\n","3337.0\n","3344.0\n","3352.0\n","3360.0\n","3369.0\n","3377.0\n","3384.0\n","3393.0\n","3401.0\n","3407.0\n","3416.0\n","3425.0\n","3433.0\n","3441.0\n","3450.0\n","3459.0\n","3466.0\n","3475.0\n","3481.0\n","3488.0\n","3497.0\n","3504.0\n","3513.0\n","3519.0\n","3526.0\n","3536.0\n","3544.0\n","3554.0\n","3562.0\n","3568.0\n","3575.0\n","3584.0\n","3593.0\n","3602.0\n","3612.0\n","3620.0\n","3625.0\n","3634.0\n","3642.0\n","3651.0\n","3658.0\n","3667.0\n","3677.0\n","3684.0\n","3692.0\n","3698.0\n","3708.0\n","3716.0\n","3723.0\n","3731.0\n","3737.0\n","3745.0\n","3753.0\n","3761.0\n","3770.0\n","3779.0\n","3787.0\n","3794.0\n","3802.0\n","3811.0\n","3817.0\n","3823.0\n","3833.0\n","3843.0\n","3852.0\n","3857.0\n","3867.0\n","3872.0\n","3882.0\n","3891.0\n","3899.0\n","3906.0\n","3914.0\n","3923.0\n","3931.0\n","3939.0\n","3946.0\n","3954.0\n","3961.0\n","3969.0\n","3979.0\n","3989.0\n","3999.0\n","4008.0\n","4018.0\n","4026.0\n","4034.0\n","4043.0\n","4050.0\n","4057.0\n","4064.0\n","4073.0\n","4081.0\n","4089.0\n","4097.0\n","4107.0\n","4117.0\n","4124.0\n","4133.0\n","4143.0\n","4153.0\n","4162.0\n","4167.0\n","4175.0\n","4183.0\n","4189.0\n","4197.0\n","4206.0\n","4215.0\n","4223.0\n","4231.0\n","4241.0\n","4249.0\n","4257.0\n","4264.0\n","4272.0\n","4280.0\n","4288.0\n","4296.0\n","4306.0\n","4313.0\n","4319.0\n","4329.0\n","4338.0\n","4345.0\n","4352.0\n","4361.0\n","4370.0\n","4376.0\n","4384.0\n","4391.0\n","4400.0\n","4409.0\n","4418.0\n","4423.0\n","4431.0\n","4438.0\n","4446.0\n","4455.0\n","4462.0\n","4470.0\n","4478.0\n","4487.0\n","4494.0\n","4501.0\n","4510.0\n","4517.0\n","4525.0\n","4533.0\n","4542.0\n","4550.0\n","4555.0\n","4564.0\n","4572.0\n","4579.0\n","4585.0\n","4593.0\n","4602.0\n","4610.0\n","4618.0\n","4627.0\n","4633.0\n","4642.0\n","4650.0\n","4660.0\n","4667.0\n","4674.0\n","4681.0\n","4690.0\n","4697.0\n","4707.0\n","4715.0\n","4723.0\n","4732.0\n","4738.0\n","4745.0\n","4753.0\n","4760.0\n","4766.0\n","4775.0\n","4783.0\n","4792.0\n","4800.0\n","4805.0\n","4813.0\n","4822.0\n","4830.0\n","4838.0\n","4846.0\n","4855.0\n","4864.0\n","4871.0\n","4878.0\n","4884.0\n","4894.0\n","4902.0\n","4911.0\n","4919.0\n","4927.0\n","4935.0\n","4943.0\n","4950.0\n","4959.0\n","4968.0\n","4975.0\n","4982.0\n","4990.0\n","4997.0\n","5006.0\n","5014.0\n","5020.0\n","5030.0\n","5038.0\n","5045.0\n","5053.0\n","5061.0\n","5067.0\n","5077.0\n","5086.0\n","5094.0\n","5103.0\n","5109.0\n","5117.0\n","5123.0\n","5130.0\n","5138.0\n","5145.0\n","5152.0\n","5159.0\n","5163.0\n","5171.0\n","5179.0\n","5189.0\n","5196.0\n","5204.0\n","5213.0\n","5221.0\n","5228.0\n","5238.0\n","5244.0\n","5252.0\n","5259.0\n","5267.0\n","5276.0\n","5282.0\n","5291.0\n","5299.0\n","5305.0\n","5312.0\n","5319.0\n","5325.0\n","5333.0\n","5341.0\n","5349.0\n","5356.0\n","5363.0\n","5372.0\n","5380.0\n","5388.0\n","5397.0\n","5404.0\n","5413.0\n","5420.0\n","5428.0\n","5437.0\n","5445.0\n","5453.0\n","5460.0\n","5468.0\n","5478.0\n","5485.0\n","5493.0\n","5503.0\n","5509.0\n","5517.0\n","5526.0\n","5535.0\n","5542.0\n","5549.0\n","5559.0\n","5569.0\n","5578.0\n","5585.0\n","5592.0\n","5601.0\n","5609.0\n","5619.0\n","5627.0\n","5633.0\n","5643.0\n","5650.0\n","5657.0\n","5664.0\n","5672.0\n","5680.0\n","5688.0\n","5697.0\n","5706.0\n","5715.0\n","5722.0\n","5728.0\n","5735.0\n","5744.0\n","5751.0\n","5758.0\n","5767.0\n","5774.0\n","5783.0\n","5791.0\n","5800.0\n","5809.0\n","5816.0\n","5826.0\n","5832.0\n","5840.0\n","5848.0\n","5854.0\n","5863.0\n","5872.0\n","5879.0\n","5886.0\n","5892.0\n","5900.0\n","5905.0\n","5911.0\n","5918.0\n","5928.0\n","5935.0\n","5944.0\n","5952.0\n","5959.0\n","5965.0\n","5971.0\n","5977.0\n","5985.0\n","5992.0\n","6001.0\n","6011.0\n","6018.0\n","6024.0\n","6033.0\n","6041.0\n","6049.0\n","6059.0\n","6065.0\n","6074.0\n","6081.0\n","6089.0\n","6099.0\n","6108.0\n","6117.0\n","6125.0\n","6131.0\n","6140.0\n","6149.0\n","6158.0\n","6167.0\n","6173.0\n","6182.0\n","6190.0\n","6200.0\n","6208.0\n","6214.0\n","6220.0\n","6227.0\n","6237.0\n","6247.0\n","6257.0\n","6264.0\n","6272.0\n","6280.0\n","6289.0\n","6298.0\n","6304.0\n","6309.0\n","6319.0\n","6327.0\n","6334.0\n","6344.0\n","6353.0\n","6361.0\n","6371.0\n","6380.0\n","6389.0\n","6397.0\n","6405.0\n","6412.0\n","6422.0\n","6431.0\n","6441.0\n","6451.0\n","6460.0\n","6468.0\n","6477.0\n","6486.0\n","6494.0\n","6504.0\n","6512.0\n","6522.0\n","6531.0\n","6539.0\n","6546.0\n","6554.0\n","6562.0\n","6571.0\n","6576.0\n","6583.0\n","6590.0\n","6597.0\n","6606.0\n","6613.0\n","6618.0\n","6624.0\n","6633.0\n","6639.0\n","6647.0\n","6654.0\n","6661.0\n","6669.0\n","6676.0\n","6685.0\n","6693.0\n","6702.0\n","6711.0\n","6718.0\n","6727.0\n","6737.0\n","6745.0\n","6752.0\n","6760.0\n","6766.0\n","6776.0\n","6784.0\n","6790.0\n","6800.0\n","6808.0\n","6815.0\n","6823.0\n","6830.0\n","6837.0\n","6845.0\n","6851.0\n","6857.0\n","6863.0\n","6873.0\n","6879.0\n","6886.0\n","6893.0\n","6902.0\n","6910.0\n","6920.0\n","6926.0\n","6933.0\n","6940.0\n","6946.0\n","6954.0\n","6962.0\n","6972.0\n","6980.0\n","6985.0\n","6994.0\n","7002.0\n","7012.0\n","7020.0\n","7026.0\n","7031.0\n","7041.0\n","7049.0\n","7056.0\n","7066.0\n","7074.0\n","7084.0\n","7094.0\n","7102.0\n","7111.0\n","7117.0\n","7126.0\n","7133.0\n","7143.0\n","7150.0\n","7159.0\n","7168.0\n","7175.0\n","7183.0\n","7189.0\n","7195.0\n","7205.0\n","7214.0\n","7221.0\n","7229.0\n","7236.0\n","7242.0\n","7249.0\n","7257.0\n","7265.0\n","7274.0\n","7283.0\n","7291.0\n","7299.0\n","7306.0\n","7315.0\n","7324.0\n","7330.0\n","7339.0\n","7348.0\n","7356.0\n","7364.0\n","7372.0\n","7378.0\n","7387.0\n","7397.0\n","7407.0\n","7417.0\n","7425.0\n","7430.0\n","7436.0\n","7443.0\n","7453.0\n","7461.0\n","7470.0\n","7479.0\n","7488.0\n","7495.0\n","7505.0\n","7513.0\n","7518.0\n","7527.0\n","7535.0\n","7543.0\n","7553.0\n","7558.0\n","7567.0\n","7575.0\n","7583.0\n","7591.0\n","7597.0\n","7605.0\n","7613.0\n","7621.0\n","7629.0\n","7636.0\n","7644.0\n","7652.0\n","7661.0\n","7670.0\n","7676.0\n","7685.0\n","7692.0\n","7700.0\n","7708.0\n","7717.0\n","7725.0\n","7733.0\n","7739.0\n","7746.0\n","7754.0\n","7762.0\n","7770.0\n","7778.0\n","7786.0\n","7794.0\n","7801.0\n","7809.0\n","7815.0\n","7821.0\n","7829.0\n","7838.0\n","7848.0\n","7855.0\n","7862.0\n","7869.0\n","7878.0\n","7886.0\n","7894.0\n","7901.0\n","7909.0\n","7916.0\n","7923.0\n","7932.0\n","7937.0\n","7946.0\n","7953.0\n","7962.0\n","7972.0\n","7981.0\n","7988.0\n","7996.0\n","8004.0\n","8012.0\n","8020.0\n","8028.0\n","8034.0\n","8043.0\n","8051.0\n","8061.0\n","8070.0\n","8079.0\n","8087.0\n","8095.0\n","8104.0\n","8112.0\n","8121.0\n","8129.0\n","8135.0\n","8144.0\n","8150.0\n","8158.0\n","8166.0\n","8175.0\n","8180.0\n","8186.0\n","8192.0\n","8200.0\n","8208.0\n","8217.0\n","8225.0\n","8233.0\n","8242.0\n","8249.0\n","8257.0\n","8261.0\n","8268.0\n","8277.0\n","8284.0\n","8294.0\n","8300.0\n","8307.0\n","8315.0\n","8321.0\n","8330.0\n","8339.0\n","8346.0\n","8353.0\n","8363.0\n","8371.0\n","8380.0\n","8388.0\n","8394.0\n","8400.0\n","8407.0\n","8415.0\n","8424.0\n","8430.0\n","8438.0\n","8447.0\n","8455.0\n","8463.0\n","8472.0\n","8479.0\n","8488.0\n","8495.0\n","8502.0\n","8510.0\n","8519.0\n","8528.0\n","8537.0\n","8544.0\n","8551.0\n","8556.0\n","8563.0\n","8572.0\n","8579.0\n","8587.0\n","8595.0\n","8601.0\n","8608.0\n","8616.0\n","8623.0\n","8631.0\n","8635.0\n","8641.0\n","8649.0\n","8655.0\n","8662.0\n","8671.0\n","8680.0\n","8688.0\n","8696.0\n","8704.0\n","8714.0\n","8722.0\n","8729.0\n","8739.0\n","8746.0\n","8756.0\n","8764.0\n","8771.0\n","8780.0\n","8787.0\n","8794.0\n","8802.0\n","8810.0\n","8818.0\n","8824.0\n","8833.0\n","8840.0\n","8849.0\n","8859.0\n","8866.0\n","8874.0\n","8883.0\n","8891.0\n","8899.0\n","8903.0\n","8910.0\n","8919.0\n","8924.0\n","8934.0\n","8940.0\n","8948.0\n","8952.0\n","8960.0\n","8969.0\n","8977.0\n","8986.0\n","8996.0\n","9006.0\n","9013.0\n","9023.0\n","9033.0\n","9042.0\n","9049.0\n","9057.0\n","9064.0\n","9072.0\n","9078.0\n","9086.0\n","9094.0\n","9101.0\n","9110.0\n","9117.0\n","9125.0\n","9134.0\n","9144.0\n","9151.0\n","9158.0\n","9162.0\n","9168.0\n","9177.0\n","9182.0\n","9188.0\n","9196.0\n","9205.0\n","9212.0\n","9218.0\n","9225.0\n","9234.0\n","9243.0\n","9251.0\n","9258.0\n","9268.0\n","9277.0\n","9287.0\n","9295.0\n","9305.0\n","9313.0\n","9320.0\n","9327.0\n","9335.0\n","9342.0\n","9349.0\n","9359.0\n","9367.0\n","9375.0\n","9381.0\n","9385.0\n","9393.0\n","9399.0\n","9407.0\n","9415.0\n","9422.0\n","9431.0\n","9438.0\n","9447.0\n","9454.0\n","9462.0\n","9470.0\n","9478.0\n","9486.0\n","9493.0\n","9503.0\n","9511.0\n","9520.0\n","9528.0\n","9535.0\n","9544.0\n","9551.0\n","9558.0\n","9562.0\n","9570.0\n","9578.0\n","9587.0\n","9595.0\n","9602.0\n","9608.0\n","9617.0\n","9624.0\n","9632.0\n","9639.0\n","9649.0\n","9654.0\n","9661.0\n","9668.0\n","9675.0\n","9682.0\n","9690.0\n","9699.0\n","9703.0\n","9711.0\n","9720.0\n","9727.0\n","9735.0\n","9745.0\n","9751.0\n","9759.0\n","9765.0\n","9773.0\n","9782.0\n","9791.0\n","9799.0\n","9804.0\n","9812.0\n","9819.0\n","9827.0\n","9837.0\n","9846.0\n","9855.0\n","9863.0\n","9870.0\n","9879.0\n","9887.0\n","9895.0\n","9904.0\n","9913.0\n","9923.0\n","9931.0\n","9938.0\n","9946.0\n","9954.0\n","9963.0\n","9971.0\n","9981.0\n","9988.0\n","9998.0\n","10005.0\n","10014.0\n","10022.0\n","10029.0\n","10036.0\n","10041.0\n","10050.0\n","10059.0\n","10068.0\n","10076.0\n","10082.0\n","10089.0\n","10098.0\n","10105.0\n","10111.0\n","10121.0\n","10129.0\n","10137.0\n","10143.0\n","10151.0\n","10159.0\n","10166.0\n","10175.0\n","10184.0\n","10192.0\n","10199.0\n","10208.0\n","10218.0\n","10228.0\n","10237.0\n","10245.0\n","10255.0\n","10263.0\n","10271.0\n","10281.0\n","10288.0\n","10296.0\n","10304.0\n","10313.0\n","10321.0\n","10329.0\n","10335.0\n","10343.0\n","10352.0\n","10361.0\n","10366.0\n","10374.0\n","10381.0\n","10388.0\n","10393.0\n","10400.0\n","10409.0\n","10416.0\n","10424.0\n","10431.0\n","10438.0\n","10446.0\n","10455.0\n","10462.0\n","10470.0\n","10477.0\n","10486.0\n","10495.0\n","10504.0\n","10514.0\n","10523.0\n","10531.0\n","10539.0\n","10547.0\n","10556.0\n","10562.0\n","10571.0\n","10580.0\n","10588.0\n","10596.0\n","10605.0\n","10615.0\n","10622.0\n","10632.0\n","10642.0\n","10651.0\n","10660.0\n","10668.0\n","10675.0\n","10685.0\n","10691.0\n","10698.0\n","10705.0\n","10712.0\n","10721.0\n","10729.0\n","10737.0\n","10744.0\n","10754.0\n","10761.0\n","10771.0\n","10780.0\n","10789.0\n","10797.0\n","10807.0\n","10815.0\n","10822.0\n","10831.0\n","10841.0\n","10850.0\n","10856.0\n","10866.0\n","10874.0\n","10884.0\n","10892.0\n","10901.0\n","10908.0\n","10915.0\n","10925.0\n","10933.0\n","10941.0\n","10950.0\n","10959.0\n","10967.0\n","10975.0\n","10981.0\n","10986.0\n","10995.0\n","11004.0\n","11014.0\n","11020.0\n","11028.0\n","11036.0\n","11044.0\n","11051.0\n","11060.0\n","11068.0\n","11074.0\n","11080.0\n","11089.0\n","11097.0\n","11106.0\n","11114.0\n","11121.0\n","11129.0\n","11136.0\n","11144.0\n","11154.0\n","11161.0\n","11170.0\n","11178.0\n","11186.0\n","11192.0\n","11200.0\n","11209.0\n","11218.0\n","11224.0\n","11232.0\n","11238.0\n","11248.0\n","11255.0\n","11263.0\n","11272.0\n","11278.0\n","11284.0\n","11292.0\n","11301.0\n","11309.0\n","11318.0\n","11327.0\n","11336.0\n","11342.0\n","11351.0\n","11359.0\n","11368.0\n","11376.0\n","11384.0\n","11390.0\n","11397.0\n","11405.0\n","11412.0\n","11419.0\n","11428.0\n","11437.0\n","11445.0\n","11454.0\n","11461.0\n","11469.0\n","11476.0\n","11486.0\n","11491.0\n","Epoch 0: val_loss = 0.2039, val_acc = 7.9358\n","Epoch 1: train_loss = 0.1245, train_acc = 9.5699\n","9.0\n","17.0\n","26.0\n","35.0\n","43.0\n","52.0\n","61.0\n","67.0\n","76.0\n","84.0\n","87.0\n","91.0\n","99.0\n","108.0\n","112.0\n","120.0\n","129.0\n","138.0\n","146.0\n","155.0\n","163.0\n","171.0\n","180.0\n","188.0\n","196.0\n","206.0\n","212.0\n","220.0\n","229.0\n","237.0\n","245.0\n","255.0\n","262.0\n","270.0\n","279.0\n","287.0\n","295.0\n","304.0\n","314.0\n","321.0\n","330.0\n","336.0\n","346.0\n","353.0\n","360.0\n","367.0\n","375.0\n","382.0\n","392.0\n","399.0\n","407.0\n","415.0\n","423.0\n","432.0\n","439.0\n","445.0\n","454.0\n","462.0\n","471.0\n","480.0\n","490.0\n","498.0\n","507.0\n","516.0\n","522.0\n","531.0\n","539.0\n","545.0\n","555.0\n","565.0\n","574.0\n","583.0\n","592.0\n","600.0\n","607.0\n","616.0\n","625.0\n","632.0\n","640.0\n","647.0\n","655.0\n","664.0\n","673.0\n","681.0\n","691.0\n","699.0\n","709.0\n","718.0\n","726.0\n","735.0\n","742.0\n","751.0\n","759.0\n","769.0\n","775.0\n","785.0\n","791.0\n","800.0\n","809.0\n","816.0\n","824.0\n","833.0\n","840.0\n","848.0\n","857.0\n","864.0\n","872.0\n","881.0\n","887.0\n","896.0\n","905.0\n","914.0\n","923.0\n","930.0\n","939.0\n","947.0\n","955.0\n","963.0\n","971.0\n","979.0\n","986.0\n","993.0\n","1001.0\n","1010.0\n","1016.0\n","1026.0\n","1032.0\n","1041.0\n","1049.0\n","1058.0\n","1064.0\n","1073.0\n","1081.0\n","1088.0\n","1094.0\n","1103.0\n","1113.0\n","1119.0\n","1125.0\n","1132.0\n","1140.0\n","1147.0\n","1156.0\n","1164.0\n","1170.0\n","1179.0\n","1188.0\n","1196.0\n","1205.0\n","1214.0\n","1220.0\n","1228.0\n","1236.0\n","1246.0\n","1255.0\n","1262.0\n","1270.0\n","1276.0\n","1285.0\n","1294.0\n","1301.0\n","1309.0\n","1315.0\n","1324.0\n","1331.0\n","1339.0\n","1346.0\n","1355.0\n","1361.0\n","1370.0\n","1378.0\n","1387.0\n","1392.0\n","1401.0\n","1410.0\n","1417.0\n","1424.0\n","1431.0\n","1438.0\n","1446.0\n","1453.0\n","1461.0\n","1468.0\n","1473.0\n","1482.0\n","1490.0\n","1498.0\n","1507.0\n","1516.0\n","1525.0\n","1532.0\n","1541.0\n","1548.0\n","1558.0\n","1568.0\n","1577.0\n","1583.0\n","1591.0\n","1599.0\n","1607.0\n","1616.0\n","1625.0\n","1635.0\n","1644.0\n","1651.0\n","1658.0\n","1665.0\n","1673.0\n","1683.0\n","1691.0\n","1698.0\n","1707.0\n","1715.0\n","1723.0\n","1729.0\n","1736.0\n","1743.0\n","1750.0\n","1758.0\n","1765.0\n","1773.0\n","1779.0\n","1788.0\n","1797.0\n","1805.0\n","1813.0\n","1822.0\n","1828.0\n","1836.0\n","1844.0\n","1853.0\n","1862.0\n","1869.0\n","1877.0\n","1886.0\n","1894.0\n","1902.0\n","1912.0\n","1919.0\n","1927.0\n","1936.0\n","1944.0\n","1953.0\n","1962.0\n","1968.0\n","1978.0\n","1987.0\n","1996.0\n","2006.0\n","2014.0\n","2021.0\n","2031.0\n","2041.0\n","2049.0\n","2058.0\n","2064.0\n","2071.0\n","2077.0\n","2086.0\n","2096.0\n","2104.0\n","2112.0\n","2120.0\n","2129.0\n","2138.0\n","2144.0\n","2151.0\n","2159.0\n","2167.0\n","2175.0\n","2183.0\n","2190.0\n","2194.0\n","2203.0\n","2208.0\n","2215.0\n","2222.0\n","2229.0\n","2237.0\n","2245.0\n","2255.0\n","2261.0\n","2270.0\n","2276.0\n","2286.0\n","2295.0\n","2301.0\n","2309.0\n","2315.0\n","2323.0\n","2331.0\n","2338.0\n","2344.0\n","2353.0\n","2362.0\n","2367.0\n","2375.0\n","2381.0\n","2388.0\n","2394.0\n","2403.0\n","2413.0\n","2420.0\n","2428.0\n","2435.0\n","2443.0\n","2452.0\n","2460.0\n","2469.0\n","2476.0\n","2483.0\n","2490.0\n","2498.0\n","2506.0\n","2515.0\n","2522.0\n","2530.0\n","2539.0\n","2545.0\n","2552.0\n","2560.0\n","2570.0\n","2577.0\n","2587.0\n","2595.0\n","2601.0\n","2609.0\n","2619.0\n","2627.0\n","2634.0\n","2641.0\n","2651.0\n","2660.0\n","2670.0\n","2678.0\n","2685.0\n","2692.0\n","2702.0\n","2708.0\n","2715.0\n","2722.0\n","2729.0\n","2738.0\n","2745.0\n","2754.0\n","2762.0\n","2770.0\n","2777.0\n","2785.0\n","2792.0\n","2801.0\n","2809.0\n","2818.0\n","2828.0\n","2835.0\n","2842.0\n","2848.0\n","2858.0\n","2864.0\n","2874.0\n","2882.0\n","2887.0\n","2897.0\n","2904.0\n","2913.0\n","2920.0\n","2927.0\n","2935.0\n","2941.0\n","2950.0\n","2958.0\n","2966.0\n","2973.0\n","2980.0\n","2987.0\n","2994.0\n","3002.0\n","3009.0\n","3017.0\n","3026.0\n","3034.0\n","3040.0\n","3047.0\n","3054.0\n","3062.0\n","3070.0\n","3076.0\n","3083.0\n","3092.0\n","3100.0\n","3109.0\n","3117.0\n","3125.0\n","3134.0\n","3144.0\n","3153.0\n","3160.0\n","3168.0\n","3177.0\n","3186.0\n","3194.0\n","3201.0\n","3210.0\n","3219.0\n","3227.0\n","3236.0\n","3243.0\n","3252.0\n","3260.0\n","3270.0\n","3276.0\n","3282.0\n","3291.0\n","3298.0\n","3303.0\n","3310.0\n","3319.0\n","3329.0\n","3336.0\n","3344.0\n","3351.0\n","3359.0\n","3367.0\n","3375.0\n","3384.0\n","3392.0\n","3398.0\n","3405.0\n","3411.0\n","3417.0\n","3426.0\n","3434.0\n","3443.0\n","3449.0\n","3457.0\n","3467.0\n","3474.0\n","3481.0\n","3489.0\n","3499.0\n","3503.0\n","3510.0\n","3520.0\n","3528.0\n","3537.0\n","3545.0\n","3553.0\n","3562.0\n","3570.0\n","3575.0\n","3582.0\n","3592.0\n","3599.0\n","3608.0\n","3616.0\n","3623.0\n","3629.0\n","3638.0\n","3647.0\n","3654.0\n","3662.0\n","3672.0\n","3679.0\n","3688.0\n","3698.0\n","3706.0\n","3714.0\n","3724.0\n","3730.0\n","3738.0\n","3745.0\n","3754.0\n","3761.0\n","3770.0\n","3778.0\n","3786.0\n","3796.0\n","3804.0\n","3813.0\n","3819.0\n","3826.0\n","3832.0\n","3842.0\n","3851.0\n","3859.0\n","3866.0\n","3875.0\n","3882.0\n","3889.0\n","3898.0\n","3904.0\n","3911.0\n","3919.0\n","3928.0\n","3937.0\n","3943.0\n","3952.0\n","3961.0\n","3969.0\n","3978.0\n","3986.0\n","3995.0\n","4004.0\n","4012.0\n","4018.0\n","4027.0\n","4035.0\n","4043.0\n","4053.0\n","4060.0\n","4069.0\n","4078.0\n","4086.0\n","4094.0\n","4101.0\n","4108.0\n","4117.0\n","4124.0\n","4131.0\n","4141.0\n","4149.0\n","4159.0\n","4166.0\n","4175.0\n","4184.0\n","4194.0\n","4201.0\n","4211.0\n","4220.0\n","4227.0\n","4234.0\n","4242.0\n","4249.0\n","4259.0\n","4265.0\n","4275.0\n","4284.0\n","4291.0\n","4298.0\n","4306.0\n","4314.0\n","4321.0\n","4330.0\n","4338.0\n","4345.0\n","4351.0\n","4358.0\n","4367.0\n","4376.0\n","4382.0\n","4391.0\n","4401.0\n","4410.0\n","4416.0\n","4426.0\n","4433.0\n","4441.0\n","4445.0\n","4454.0\n","4462.0\n","4470.0\n","4478.0\n","4487.0\n","4495.0\n","4503.0\n","4512.0\n","4519.0\n","4526.0\n","4535.0\n","4545.0\n","4552.0\n","4560.0\n","4568.0\n","4576.0\n","4585.0\n","4593.0\n","4600.0\n","4607.0\n","4613.0\n","4622.0\n","4631.0\n","4640.0\n","4646.0\n","4655.0\n","4662.0\n","4670.0\n","4675.0\n","4684.0\n","4694.0\n","4701.0\n","4709.0\n","4714.0\n","4723.0\n","4732.0\n","4742.0\n","4751.0\n","4758.0\n","4765.0\n","4774.0\n","4784.0\n","4790.0\n","4799.0\n","4807.0\n","4813.0\n","4820.0\n","4826.0\n","4834.0\n","4840.0\n","4847.0\n","4854.0\n","4861.0\n","4869.0\n","4877.0\n","4885.0\n","4894.0\n","4901.0\n","4909.0\n","4917.0\n","4926.0\n","4934.0\n","4943.0\n","4951.0\n","4960.0\n","4968.0\n","4976.0\n","4986.0\n","4993.0\n","5000.0\n","5008.0\n","5015.0\n","5020.0\n","5029.0\n","5036.0\n","5043.0\n","5050.0\n","5060.0\n","5066.0\n","5073.0\n","5081.0\n","5089.0\n","5096.0\n","5104.0\n","5113.0\n","5120.0\n","5129.0\n","5136.0\n","5141.0\n","5148.0\n","5155.0\n","5163.0\n","5171.0\n","5179.0\n","5189.0\n","5197.0\n","5206.0\n","5215.0\n","5224.0\n","5230.0\n","5239.0\n","5248.0\n","5257.0\n","5264.0\n","5270.0\n","5278.0\n","5285.0\n","5295.0\n","5303.0\n","5312.0\n","5319.0\n","5328.0\n","5337.0\n","5343.0\n","5350.0\n","5356.0\n","5363.0\n","5369.0\n","5377.0\n","5386.0\n","5393.0\n","5401.0\n","5411.0\n","5421.0\n","5429.0\n","5437.0\n","5445.0\n","5454.0\n","5462.0\n","5469.0\n","5477.0\n","5485.0\n","5492.0\n","5500.0\n","5509.0\n","5518.0\n","5525.0\n","5532.0\n","5541.0\n","5550.0\n","5558.0\n","5566.0\n","5573.0\n","5582.0\n","5588.0\n","5595.0\n","5603.0\n","5611.0\n","5619.0\n","5629.0\n","5638.0\n","5645.0\n","5651.0\n","5659.0\n","5666.0\n","5674.0\n","5684.0\n","5693.0\n","5703.0\n","5710.0\n","5716.0\n","5724.0\n","5731.0\n","5737.0\n","5744.0\n","5753.0\n","5763.0\n","5771.0\n","5779.0\n","5789.0\n","5798.0\n","5806.0\n","5816.0\n","5823.0\n","5832.0\n","5840.0\n","5846.0\n","5854.0\n","5863.0\n","5871.0\n","5878.0\n","5887.0\n","5895.0\n","5903.0\n","5909.0\n","5916.0\n","5925.0\n","5934.0\n","5943.0\n","5953.0\n","5963.0\n","5971.0\n","5979.0\n","5988.0\n","5997.0\n","6005.0\n","6014.0\n","6023.0\n","6032.0\n","6040.0\n","6048.0\n","6058.0\n","6067.0\n","6075.0\n","6084.0\n","6093.0\n","6099.0\n","6108.0\n","6116.0\n","6124.0\n","6130.0\n","6138.0\n","6148.0\n","6153.0\n","6161.0\n","6168.0\n","6176.0\n","6184.0\n","6193.0\n","6202.0\n","6210.0\n","6219.0\n","6225.0\n","6232.0\n","6241.0\n","6250.0\n","6258.0\n","6265.0\n","6271.0\n","6279.0\n","6286.0\n","6293.0\n","6302.0\n","6310.0\n","6317.0\n","6324.0\n","6333.0\n","6341.0\n","6348.0\n","6357.0\n","6364.0\n","6373.0\n","6381.0\n","6391.0\n","6399.0\n","6408.0\n","6417.0\n","6426.0\n","6435.0\n","6442.0\n","6452.0\n","6461.0\n","6470.0\n","6479.0\n","6488.0\n","6495.0\n","6500.0\n","6509.0\n","6518.0\n","6526.0\n","6534.0\n","6543.0\n","6551.0\n","6560.0\n","6565.0\n","6574.0\n","6581.0\n","6590.0\n","6598.0\n","6608.0\n","6616.0\n","6623.0\n","6632.0\n","6640.0\n","6648.0\n","6658.0\n","6665.0\n","6672.0\n","6679.0\n","6687.0\n","6696.0\n","6704.0\n","6710.0\n","6719.0\n","6728.0\n","6737.0\n","6746.0\n","6754.0\n","6761.0\n","6770.0\n","6777.0\n","6786.0\n","6793.0\n","6798.0\n","6805.0\n","6812.0\n","6820.0\n","6826.0\n","6835.0\n","6841.0\n","6851.0\n","6857.0\n","6864.0\n","6870.0\n","6878.0\n","6885.0\n","6895.0\n","6904.0\n","6911.0\n","6919.0\n","6928.0\n","6936.0\n","6943.0\n","6952.0\n","6959.0\n","6968.0\n","6976.0\n","6984.0\n","6992.0\n","6998.0\n","7006.0\n","7014.0\n","7023.0\n","7031.0\n","7038.0\n","7047.0\n","7053.0\n","7061.0\n","7068.0\n","7078.0\n","7086.0\n","7095.0\n","7102.0\n","7112.0\n","7120.0\n","7126.0\n","7136.0\n","7144.0\n","7153.0\n","7163.0\n","7171.0\n","7180.0\n","7186.0\n","7195.0\n","7204.0\n","7213.0\n","7221.0\n","7231.0\n","7237.0\n","7243.0\n","7250.0\n","7259.0\n","7265.0\n","7275.0\n","7281.0\n","7290.0\n","7297.0\n","7304.0\n","7314.0\n","7320.0\n","7328.0\n","7336.0\n","7343.0\n","7352.0\n","7362.0\n","7368.0\n","7375.0\n","7381.0\n","7390.0\n","7399.0\n","7405.0\n","7412.0\n","7421.0\n","7429.0\n","7437.0\n","7445.0\n","7452.0\n","7460.0\n","7469.0\n","7477.0\n","7485.0\n","7495.0\n","7503.0\n","7512.0\n","7521.0\n","7529.0\n","7538.0\n","7546.0\n","7555.0\n","7563.0\n","7572.0\n","7580.0\n","7587.0\n","7595.0\n","7603.0\n","7610.0\n","7616.0\n","7624.0\n","7634.0\n","7643.0\n","7652.0\n","7662.0\n","7670.0\n","7679.0\n","7686.0\n","7694.0\n","7701.0\n","7711.0\n","7719.0\n","7728.0\n","7734.0\n","7741.0\n","7749.0\n","7758.0\n","7767.0\n","7777.0\n","7784.0\n","7792.0\n","7800.0\n","7808.0\n","7815.0\n","7823.0\n","7830.0\n","7838.0\n","7846.0\n","7854.0\n","7862.0\n","7869.0\n","7876.0\n","7886.0\n","7894.0\n","7902.0\n","7908.0\n","7916.0\n","7923.0\n","7932.0\n","7940.0\n","7950.0\n","7958.0\n","7965.0\n","7974.0\n","7983.0\n","7990.0\n","7998.0\n","8004.0\n","8009.0\n","8016.0\n","8021.0\n","8027.0\n","8037.0\n","8044.0\n","8052.0\n","8060.0\n","8068.0\n","8075.0\n","8083.0\n","8090.0\n","8097.0\n","8102.0\n","8109.0\n","8118.0\n","8126.0\n","8135.0\n","8142.0\n","8148.0\n","8155.0\n","8161.0\n","8171.0\n","8179.0\n","8186.0\n","8195.0\n","8202.0\n","8211.0\n","8218.0\n","8227.0\n","8235.0\n","8244.0\n","8249.0\n","8259.0\n","8266.0\n","8274.0\n","8283.0\n","8290.0\n","8297.0\n","8304.0\n","8314.0\n","8324.0\n","8332.0\n","8339.0\n","8346.0\n","8354.0\n","8363.0\n","8371.0\n","8381.0\n","8389.0\n","8396.0\n","8404.0\n","8411.0\n","8420.0\n","8430.0\n","8436.0\n","8444.0\n","8452.0\n","8462.0\n","8471.0\n","8477.0\n","8487.0\n","8496.0\n","8505.0\n","8513.0\n","8522.0\n","8527.0\n","8534.0\n","8542.0\n","8549.0\n","8557.0\n","8565.0\n","8570.0\n","8580.0\n","8589.0\n","8596.0\n","8605.0\n","8612.0\n","8619.0\n","8627.0\n","8634.0\n","8643.0\n","8652.0\n","8660.0\n","8668.0\n","8676.0\n","8683.0\n","8691.0\n","8700.0\n","8707.0\n","8717.0\n","8725.0\n","8733.0\n","8740.0\n","8746.0\n","8756.0\n","8764.0\n","8773.0\n","8781.0\n","8790.0\n","8798.0\n","8807.0\n","8816.0\n","8825.0\n","8834.0\n","8840.0\n","8846.0\n","8854.0\n","8862.0\n","8869.0\n","8877.0\n","8883.0\n","8891.0\n","8900.0\n","8906.0\n","8914.0\n","8922.0\n","8930.0\n","8937.0\n","8946.0\n","8956.0\n","8965.0\n","8975.0\n","8983.0\n","8992.0\n","9001.0\n","9009.0\n","9016.0\n","9024.0\n","9032.0\n","9041.0\n","9049.0\n","9056.0\n","9064.0\n","9072.0\n","9080.0\n","9089.0\n","9098.0\n","9106.0\n","9115.0\n","9125.0\n","9133.0\n","9140.0\n","9147.0\n","9156.0\n","9164.0\n","9170.0\n","9177.0\n","9186.0\n","9195.0\n","9203.0\n","9211.0\n","9218.0\n","9224.0\n","9233.0\n","9242.0\n","9250.0\n","9257.0\n","9262.0\n","9268.0\n","9277.0\n","9284.0\n","9293.0\n","9300.0\n","9308.0\n","9316.0\n","9322.0\n","9330.0\n","9338.0\n","9345.0\n","9354.0\n","9360.0\n","9368.0\n","9376.0\n","9382.0\n","9389.0\n","9399.0\n","9408.0\n","9417.0\n","9425.0\n","9433.0\n","9440.0\n","9449.0\n","9457.0\n","9465.0\n","9475.0\n","9485.0\n","9493.0\n","9503.0\n","9510.0\n","9518.0\n","9526.0\n","9533.0\n","9540.0\n","9548.0\n","9556.0\n","9564.0\n","9574.0\n","9582.0\n","9591.0\n","9598.0\n","9606.0\n","9613.0\n","9620.0\n","9626.0\n","9636.0\n","9644.0\n","9651.0\n","9659.0\n","9664.0\n","9672.0\n","9680.0\n","9685.0\n","9694.0\n","9702.0\n","9710.0\n","9718.0\n","9724.0\n","9732.0\n","9740.0\n","9748.0\n","9754.0\n","9762.0\n","9772.0\n","9781.0\n","9790.0\n","9797.0\n","9805.0\n","9814.0\n","9821.0\n","9828.0\n","9836.0\n","9843.0\n","9850.0\n","9854.0\n","9862.0\n","9871.0\n","9879.0\n","9887.0\n","9896.0\n","9901.0\n","9907.0\n","9916.0\n","9924.0\n","9933.0\n","9942.0\n","9951.0\n","9958.0\n","9965.0\n","9975.0\n","9982.0\n","9990.0\n","9999.0\n","10008.0\n","10016.0\n","10023.0\n","10032.0\n","10041.0\n","10048.0\n","10057.0\n","10066.0\n","10074.0\n","10083.0\n","10091.0\n","10099.0\n","10106.0\n","10115.0\n","10125.0\n","10134.0\n","10140.0\n","10148.0\n","10156.0\n","10163.0\n","10169.0\n","10178.0\n","10188.0\n","10198.0\n","10204.0\n","10212.0\n","10220.0\n","10226.0\n","10232.0\n","10242.0\n","10251.0\n","10259.0\n","10266.0\n","10276.0\n","10286.0\n","10293.0\n","10301.0\n","10309.0\n","10317.0\n","10327.0\n","10336.0\n","10344.0\n","10354.0\n","10361.0\n","10370.0\n","10378.0\n","10387.0\n","10395.0\n","10405.0\n","10414.0\n","10423.0\n","10430.0\n","10440.0\n","10449.0\n","10456.0\n","10462.0\n","10469.0\n","10479.0\n","10485.0\n","10492.0\n","10499.0\n","10506.0\n","10514.0\n","10518.0\n","10525.0\n","10532.0\n","10542.0\n","10552.0\n","10560.0\n","10567.0\n","10576.0\n","10582.0\n","10589.0\n","10596.0\n","10603.0\n","10613.0\n","10622.0\n","10631.0\n","10641.0\n","10650.0\n","10660.0\n","10669.0\n","10676.0\n","10684.0\n","10692.0\n","10699.0\n","10708.0\n","10715.0\n","10724.0\n","10732.0\n","10740.0\n","10747.0\n","10757.0\n","10766.0\n","10775.0\n","10781.0\n","10789.0\n","10797.0\n","10804.0\n","10811.0\n","10819.0\n","10826.0\n","10832.0\n","10836.0\n","10845.0\n","10855.0\n","10861.0\n","10866.0\n","10874.0\n","10883.0\n","10889.0\n","10898.0\n","10907.0\n","10914.0\n","10921.0\n","10930.0\n","10939.0\n","10947.0\n","10956.0\n","10965.0\n","10973.0\n","10979.0\n","10986.0\n","10995.0\n","11004.0\n","11013.0\n","11020.0\n","11030.0\n","11038.0\n","11047.0\n","11055.0\n","11062.0\n","11072.0\n","11078.0\n","11087.0\n","11095.0\n","11103.0\n","11112.0\n","11119.0\n","11128.0\n","11136.0\n","11145.0\n","11152.0\n","11161.0\n","11170.0\n","11174.0\n","11182.0\n","11189.0\n","11196.0\n","11204.0\n","11210.0\n","11217.0\n","11226.0\n","11233.0\n","11239.0\n","11246.0\n","11256.0\n","11264.0\n","11273.0\n","11281.0\n","11289.0\n","11293.0\n","11302.0\n","11308.0\n","11316.0\n","11326.0\n","11334.0\n","11344.0\n","11353.0\n","11363.0\n","11371.0\n","11380.0\n","11389.0\n","11397.0\n","11404.0\n","11412.0\n","11421.0\n","11429.0\n","11439.0\n","11449.0\n","11457.0\n","11466.0\n","11472.0\n","11479.0\n","11489.0\n","11497.0\n","11503.0\n","11513.0\n","11522.0\n","11531.0\n","11539.0\n","11543.0\n","Epoch 1: val_loss = 0.1866, val_acc = 7.9717\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/data/zmengaf/5212/RCFDA/bert.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs\u001b[39m.\u001b[39mlogits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bck1/data/zmengaf/5212/RCFDA/bert.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m acc \u001b[39m=\u001b[39m (preds \u001b[39m==\u001b[39m labels)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\n","File \u001b[0;32m~/miniconda3/envs/rcfda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/rcfda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from transformers import AdamW, AutoModelForSequenceClassification\n","import wandb\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n","# best_loss = float('inf')  # initialize with a very large value\n","wandb.init(entity='zmengaf', project='rcfda', name='arxiv_bert')\n","wandb.config.update({\n","    \"max_length\": max_length,\n","    \"num_epoch\": num_epoch,\n","    \"batch_size\": batch_size,\n","    \"num_labels\": num_labels,\n","})\n","\n","max_val_acc = 0\n","for epoch in range(num_epoch):\n","# for epoch in range(4, 6):\n","    model.train()\n","    train_loss = 0\n","    train_correct = 0\n","    for batch_idx, batch in enumerate(train_dataloader):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        preds = torch.argmax(outputs.logits, dim=1)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        acc = (preds == labels).float().mean()\n","        train_loss += loss.item()\n","        train_correct += (outputs.logits.argmax(dim=1) == labels).float().sum().item()\n","    train_loss /= len(train_dataloader)\n","    train_acc = train_correct / len(train_dataset)\n","    print(f\"Epoch {epoch}: train_loss = {train_loss:.4f}, train_acc = {train_acc:.4f}\")\n","    \n","    model.eval()\n","    val_loss = 0\n","    val_correct = 0\n","    for batch in val_dataloader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        with torch.no_grad():\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            logits = outputs.logits\n","            preds = torch.argmax(logits, dim=1)\n","            val_loss += loss.item()\n","            val_correct += (outputs.logits.argmax(dim=1) == labels).float().sum().item()\n","            print(val_correct)\n","    val_loss /= len(val_dataloader)\n","    val_acc = val_correct / len(val_dataset)\n","    if val_acc > max_val_acc:\n","        max_val_acc = val_acc\n","        torch.save(model, f\"bert_model_arxiv_acc_{max_val_acc}.pt\")\n","    print(f\"Epoch {epoch}: val_loss = {val_loss:.4f}, val_acc = {val_acc:.4f}\")\n","    wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc})"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNwwF2Pa0Z0mHEOFD5X8nnA","provenance":[]},"kernelspec":{"display_name":"rcfda","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
